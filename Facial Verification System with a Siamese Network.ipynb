{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti-Spoofing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2696\\1918377201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mface_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two constants, one for the eye aspect ratio to indicate\n",
    "# blink and then a second constant for the number of consecutive\n",
    "# frames the eye must be below the threshold\n",
    "EYE_AR_THRESH = 0.23 #baseline\n",
    "EYE_AR_CONSEC_FRAMES = 3\n",
    "\n",
    "# eye landmarks\n",
    "eye_landmarks = \"shape_predictor_68_face_landmarks.dat\"\n",
    "# initialize the frame counters and the total number of blinks\n",
    "COUNTER = 0\n",
    "TOTAL = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 eye blink detector class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eye_blink_detector():\n",
    "    def __init__(self):\n",
    "        # cargar modelo para detecction frontal de rostros\n",
    "        self.detector_faces = dlib.get_frontal_face_detector()\n",
    "        # cargar modelo para deteccion de puntos de ojos\n",
    "        self.predictor_eyes = dlib.shape_predictor(eye_landmarks)\n",
    "\n",
    "    def eye_blink(self,gray,rect,COUNTER,TOTAL):\n",
    "        (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "        (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = self.predictor_eyes(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        # extract the left and right eye coordinates, then use the\n",
    "        # coordinates to compute the eye aspect ratio for both eyes\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        leftEAR = self.eye_aspect_ratio(leftEye)\n",
    "        rightEAR = self.eye_aspect_ratio(rightEye)\n",
    "        # average the eye aspect ratio together for both eyes\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "        # check to see if the eye aspect ratio is below the blink\n",
    "        # threshold, and if so, increment the blink frame counter\n",
    "        if ear < EYE_AR_THRESH:\n",
    "            COUNTER += 1\n",
    "        # otherwise, the eye aspect ratio is not below the blink\n",
    "        # threshold\n",
    "        else:\n",
    "            # if the eyes were closed for a sufficient number of\n",
    "            # then increment the total number of blinks\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                TOTAL += 1\n",
    "            # reset the eye frame counter\n",
    "            COUNTER = 0\n",
    "        return COUNTER,TOTAL\n",
    "\n",
    "    def eye_aspect_ratio(self,eye):\n",
    "        # compute the euclidean distances between the two sets of\n",
    "        # vertical eye landmarks (x, y)-coordinates\n",
    "        A = dist.euclidean(eye[1], eye[5])\n",
    "        B = dist.euclidean(eye[2], eye[4])\n",
    "        # compute the euclidean distance between the horizontal\n",
    "        # eye landmark (x, y)-coordinates\n",
    "        C = dist.euclidean(eye[0], eye[3])\n",
    "        # compute the eye aspect ratio\n",
    "        ear = (A + B) / (2.0 * C)\n",
    "        # return the eye aspect ratio\n",
    "        return ear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 eye blink detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rectangles2array(rectangles,image):\n",
    "    res = np.array([])\n",
    "    for box in rectangles:\n",
    "        [x0,y0,x1,y1] = max(0, box.left()), max(0, box.top()), min(box.right(), image.shape[1]), min(box.bottom(), image.shape[0])\n",
    "        new_box = np.array([x0,y0,x1,y1])\n",
    "        if res.size == 0:\n",
    "            res = np.expand_dims(new_box,axis=0)\n",
    "        else:\n",
    "            res = np.vstack((res,new_box))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_areas(boxes):\n",
    "    areas = []\n",
    "    for box in boxes:\n",
    "        x0,y0,x1,y1 = box\n",
    "        area = (y1-y0)*(x1-x0)\n",
    "        areas.append(area)\n",
    "    return areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box(img,box,match_name=[]):\n",
    "    for i in np.arange(len(box)):\n",
    "        x0,y0,x1,y1 = box[i]\n",
    "        img = cv2.rectangle(img,\n",
    "                    (x0,y0),\n",
    "                    (x1,y1),\n",
    "                    (0,255,0),3);\n",
    "        if not match_name:\n",
    "            continue\n",
    "        else:\n",
    "            cv2.putText(img, match_name[i], (x0, y0-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get User's Verification images & Eye blinking Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instancio detector\n",
    "detector = eye_blink_detector()\n",
    "# iniciar variables para el detector de parapadeo\n",
    "COUNTER = 0\n",
    "TOTAL = 0\n",
    "video_path = \"data/videos/video.mp4\" #/content/video.mp4\n",
    "image_path = \"data/images/\"\n",
    "verification_path = \"data/verification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verification_images():\n",
    "    \n",
    "    verification_video_path = verification_path + \"video/video.mp4\"\n",
    "    if not(os.path.isdir(verification_path + \"video\")):\n",
    "        os.makedirs(verification_path + \"video\")\n",
    "        \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # 저장될 영상은 mp4형식이다.\n",
    "    # 캠이 여러대인 경우 인자로 0, 1, 2, 3...을 넣어주면 된다.\n",
    "    # 캠으로부터 정보를 읽어들일 수 없는 경우 에러 메세지를 반환한다.\n",
    "    if cap.isOpened() == False:\n",
    "        print(\"Unable to read camera\")\n",
    "\n",
    "    # 캠으로부터 정보를 읽어들일 수 있으면,\n",
    "    else:\n",
    "        # 프레임의 정보 가져와 변수에 저장한다. \n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "        print(\"원을 그리듯이 고개를 돌려주세요\")\n",
    "\n",
    "        #캠으로 들어온 비디오를 따로 저장한다.\n",
    "        out = cv2.VideoWriter(verification_video_path,\n",
    "                            cv2.VideoWriter_fourcc('D', 'I', 'V', 'X'),\n",
    "                            10,\n",
    "                            (frame_width, frame_height) )\n",
    "\n",
    "\n",
    "        # 동영상은 사진을 여러장 이어서 보여주는 개념이다.\n",
    "        # 1초에 몇 장의 이미지가 들어가는지, fps(frame per second) 단위를 쓴다.\n",
    "        # 캠으로부터 이미지 한 장만을 받아올 게 아니므로, 반복문을 사용한다.\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if ret == True:\n",
    "                out.write(frame)\n",
    "                cv2.imshow('frame', frame)\n",
    "                recording_time = time.time() - start_time\n",
    "                #esc를 입력하면, 이미지를 받아오길 멈추게 한다.\n",
    "                if cv2.waitKey(1) & 0xFF == 27: # esc누르면 종료\n",
    "                    break\n",
    "                elif recording_time > 10: #최대 10초 정도까지 촬영가능\n",
    "                    print(\"time over\")\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # sql 커서와 커넥션을 다 사용하고 나면 연결을 닫아주듯이, 비디오캡쳐도 닫아준다.\n",
    "        cap.release()\n",
    "        # 파일도 더 이상 작성하지 않도록 한다.\n",
    "        out.release()\n",
    "\n",
    "        #화면에 띄운 창을 닫아준다.\n",
    "        cv2.destroyAllWindows()\n",
    "        return verification_video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want User Registration? [y or n]: y\n",
      "time over\n",
      "saved image image1.jpg\n",
      "saved image image2.jpg\n",
      "saved image image3.jpg\n",
      "saved image image4.jpg\n",
      "saved image image5.jpg\n",
      "saved image image6.jpg\n",
      "saved image image7.jpg\n",
      "saved image image8.jpg\n",
      "saved image image9.jpg\n",
      "saved image image10.jpg\n",
      "saved image image11.jpg\n",
      "saved image image12.jpg\n",
      "saved image image13.jpg\n",
      "saved image image14.jpg\n",
      "saved image image15.jpg\n",
      "saved image image16.jpg\n",
      "saved image image17.jpg\n",
      "saved image image18.jpg\n",
      "saved image image19.jpg\n",
      "saved image image20.jpg\n",
      "saved image image21.jpg\n",
      "saved image image22.jpg\n",
      "saved image image23.jpg\n",
      "saved image image24.jpg\n",
      "saved image image25.jpg\n",
      "saved image image26.jpg\n",
      "saved image image27.jpg\n",
      "saved image image28.jpg\n",
      "saved image image29.jpg\n",
      "saved image image30.jpg\n",
      "saved image image31.jpg\n",
      "saved image image32.jpg\n",
      "saved image image33.jpg\n",
      "saved image image34.jpg\n",
      "saved image image35.jpg\n",
      "saved image image36.jpg\n",
      "saved image image37.jpg\n",
      "saved image image38.jpg\n",
      "saved image image39.jpg\n",
      "saved image image40.jpg\n",
      "saved image image41.jpg\n",
      "saved image image42.jpg\n",
      "saved image image43.jpg\n",
      "saved image image44.jpg\n",
      "saved image image45.jpg\n",
      "saved image image46.jpg\n",
      "saved image image47.jpg\n",
      "saved image image48.jpg\n",
      "saved image image49.jpg\n",
      "saved image image50.jpg\n"
     ]
    }
   ],
   "source": [
    "mode = input(\"Do you want User Registration? [y or n]: \")\n",
    "if (mode == 'Y' or mode == 'y') or not(os.path.isdir(\"data/verification\")):\n",
    "    if not(os.path.isdir(\"data/verification\")):\n",
    "        os.makedirs(\"data/verification\")\n",
    "    verification_video_path = get_verification_images()\n",
    "    verification_image_path = verification_path + \"images/\"\n",
    "    if not(os.path.isdir(\"data/verification/images\")):\n",
    "        os.makedirs(\"data/verification/images\")\n",
    "    vidcap = cv2.VideoCapture(verification_video_path)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    all_frame = vidcap.get(cv2.CAP_PROP_FRAME_COUNT) #영상 총 프레임 수\n",
    "    multiplier = all_frame / 50 # 어떤 영상이든 50장 캡쳐하도록\n",
    "    count = 1\n",
    "    success = True\n",
    "    while success:\n",
    "        frameId = int(round(vidcap.get(1)))\n",
    "        success,image = vidcap.read()\n",
    "        if frameId % multiplier < 1 : # 몇 프레임 당 한번씩 캡쳐할지\n",
    "            cv2.imwrite(verification_image_path + \"image%d.jpg\" %count, image)\n",
    "            print(\"saved image image%d.jpg\" % count)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- video -----------------------------\n",
    "# Eye blinking detection & Login\n",
    "\n",
    "vs = cv2.VideoCapture(0) #VideoStream(src=0).start()\n",
    "antiSpoofing = True\n",
    "\n",
    "frame_width = int(vs.get(3))\n",
    "frame_height = int(vs.get(4))\n",
    "\n",
    "#캠으로 들어온 비디오를 따로 저장한다.\n",
    "out = cv2.VideoWriter(video_path,\n",
    "                    cv2.VideoWriter_fourcc('D', 'I', 'V', 'X'),\n",
    "                    20,\n",
    "                    (frame_width, frame_height) )\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"눈을 깜박여주세요\\n\")\n",
    "while True:\n",
    "    star_time = time.time()\n",
    "    ret, im = vs.read()\n",
    "    if ret == True:\n",
    "        out.write(im)\n",
    "        cv2.imshow('frame', im)\n",
    "        im = cv2.flip(im, 1)\n",
    "        im = imutils.resize(im, width=720)\n",
    "        gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        # detectar_rostro    \n",
    "        rectangles = detector.detector_faces(gray, 0)\n",
    "        boxes_face = convert_rectangles2array(rectangles,im)\n",
    "        if len(boxes_face)!=0:\n",
    "            # seleccionar el rostro con mas area\n",
    "            areas = get_areas(boxes_face)\n",
    "            index = np.argmax(areas)\n",
    "            rectangles = rectangles[index]\n",
    "            boxes_face = np.expand_dims(boxes_face[index],axis=0)\n",
    "            # blinks_detector\n",
    "            COUNTER,TOTAL = detector.eye_blink(gray,rectangles,COUNTER,TOTAL)\n",
    "            # agregar bounding box\n",
    "            img_post = bounding_box(im,boxes_face,['blinks: {}'.format(TOTAL)])\n",
    "            if TOTAL > 1 :\n",
    "                break\n",
    "        else:\n",
    "            img_post = im \n",
    "        # visualizacion \n",
    "        end_time = time.time() - star_time    \n",
    "        FPS = 1/end_time\n",
    "        cv2.putText(img_post,f\"FPS: {round(FPS,3)}\",(10,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,255),2)\n",
    "        cv2.imshow('blink_detection',img_post)\n",
    "        recording_time = time.time() - start_time\n",
    "        if TOTAL > 1 : #한번 이상 눈깜박거림\n",
    "            break\n",
    "        elif recording_time > 7: #약 7초 내에 blink감지 안되면\n",
    "            antiSpoofing = False\n",
    "            print(\"time over\")\n",
    "            break\n",
    "\n",
    "# sql 커서와 커넥션을 다 사용하고 나면 연결을 닫아주듯이, 비디오캡쳐도 닫아준다.\n",
    "vs.release()\n",
    "# 파일도 더 이상 작성하지 않도록 한다.\n",
    "out.release()\n",
    "\n",
    "#화면에 띄운 창을 닫아준다.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS\n",
      "saved image image1.jpg\n"
     ]
    }
   ],
   "source": [
    "if antiSpoofing:\n",
    "    print(\"PASS\")\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    multiplier = fps*20\n",
    "    count = 1\n",
    "    success = True\n",
    "    while success:\n",
    "        frameId = int(round(vidcap.get(1)))\n",
    "        success,image = vidcap.read()\n",
    "        if frameId % multiplier < 1 :\n",
    "            cv2.imwrite(image_path + \"image%d.jpg\" %count, image)\n",
    "            print(\"saved image image%d.jpg\" % count)\n",
    "            count += 1\n",
    "else:\n",
    "    print(\"Spoofing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.isfile(image_path + \"1.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDQ6jEz5V4eu"
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tj8dXV0SZBQs",
    "outputId": "dbaa7f69-af68-4e25-fc7f-e374813e173f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cydfsp9DV4ew"
   },
   "source": [
    "## 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nOMaHk0wV4ew",
    "outputId": "ab9e94b1-0961-4c53-aef9-cc0954ba7e6e"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqauiOD2V4ex"
   },
   "source": [
    "## 1.2 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UHrLB2sV4ex"
   },
   "outputs": [],
   "source": [
    "# Import standard dependencies\n",
    "#import cv2\n",
    "#import os\n",
    "import random\n",
    "#import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8LciVxeYV4ey"
   },
   "outputs": [],
   "source": [
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrqg_9p8V4ey"
   },
   "source": [
    "## 1.3 Set GPU Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arri1pGgV4ey"
   },
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpbQ1x78V4ez"
   },
   "source": [
    "## 1.4 Create Folder Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2wK3AIbdTc1"
   },
   "outputs": [],
   "source": [
    "PATH = '/content/gdrive/Shareddrives/Face_Recognition'\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5mhqsAzV4ez"
   },
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative2')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "77ZVGEKYV4ez",
    "outputId": "68c774c2-2fbd-4717-ac50-b60932b734db"
   },
   "outputs": [],
   "source": [
    "# Make the directories\n",
    "# os.makedirs(POS_PATH)\n",
    "# os.makedirs(NEG_PATH)\n",
    "# os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShpulVq6h_wO",
    "outputId": "fe75143c-0005-40bf-df60-7cfdb0a33170"
   },
   "outputs": [],
   "source": [
    "file_list = os.listdir(NEG_PATH) \n",
    "\n",
    "print (len(file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BYrba-wV4ez"
   },
   "source": [
    "# 2. Collect Positives and Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApLPHkXiV4e0"
   },
   "source": [
    "## 2.1 Untar Labelled Faces in the Wild Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcCEaGuZV4e0"
   },
   "outputs": [],
   "source": [
    "# http://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cy7Fq4E9V4e0",
    "outputId": "8be6f795-c788-4c36-b3d3-a34620b95738"
   },
   "outputs": [],
   "source": [
    "# Uncompress Tar GZ Labelled Faces in the Wild Dataset\n",
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYu8d3M9V4e1"
   },
   "outputs": [],
   "source": [
    "# Move LFW Images to the following repository data/negative\n",
    "# for directory in os.listdir('lfw'):\n",
    "#     for file in os.listdir(os.path.join('lfw', directory)):\n",
    "#         EX_PATH = os.path.join('lfw', directory, file)\n",
    "#         NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "#         os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nYZ5FadV4e1"
   },
   "source": [
    "## 2.2 Collect Positive and Anchor Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9SDYMPUV4e1"
   },
   "outputs": [],
   "source": [
    "# Import uuid library to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ZTQAJjsBV4e1",
    "outputId": "6b7ae0c0-f1ea-4432-8bcb-5c7c5b4eda89"
   },
   "outputs": [],
   "source": [
    "os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7C59WsO3V4e2"
   },
   "outputs": [],
   "source": [
    "# # Establish a connection to the webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# while cap.isOpened(): \n",
    "#     ret, frame = cap.read()\n",
    "   \n",
    "#     # Cut down frame to 250x250px\n",
    "#     frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "#     # Collect anchors \n",
    "#     if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "#         # Create the unique file path \n",
    "#         imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "#         # Write out anchor image\n",
    "#         cv2.imwrite(imgname, frame)\n",
    "    \n",
    "#     # Collect positives\n",
    "#     if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "#         # Create the unique file path \n",
    "#         imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "#         # Write out positive image\n",
    "#         cv2.imwrite(imgname, frame)\n",
    "    \n",
    "#     # Show image back to screen\n",
    "#     cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "#     # Breaking gracefully\n",
    "#     if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "#         break\n",
    "        \n",
    "# # Release the webcam\n",
    "# cap.release()\n",
    "# # Close the image show frame\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "5uvZViZDV4e2",
    "outputId": "9a7b24b0-fe48-4940-8c1d-93eddf567f1c"
   },
   "outputs": [],
   "source": [
    "#plt.imshow(frame[120:120+250,200:200+250, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8brJapfUV4e2"
   },
   "source": [
    "# 2.x NEW - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJodneNdV4e3"
   },
   "outputs": [],
   "source": [
    "# def data_aug(img):\n",
    "#     data = []\n",
    "#     for i in range(9):\n",
    "#         img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "#         img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
    "#         # img = tf.image.stateless_random_crop(img, size=(20,20,3), seed=(1,2))\n",
    "#         img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "#         img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "#         img = tf.image.stateless_random_saturation(img, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "            \n",
    "#         data.append(img)\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmb2VITkV4e3"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bChExbsV4e3"
   },
   "outputs": [],
   "source": [
    "# img_path = os.path.join(ANC_PATH, 'a386326c-59f3-11ed-bb8e-0242ac1c0002.jpg')\n",
    "# img = cv2.imread(img_path)\n",
    "# augmented_images = data_aug(img)\n",
    "\n",
    "# for image in augmented_images:\n",
    "#     cv2.imwrite(os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVeUnUCZV4e4"
   },
   "outputs": [],
   "source": [
    "# for file_name in os.listdir(os.path.join(POS_PATH)):\n",
    "#     img_path = os.path.join(POS_PATH, file_name)\n",
    "#     img = cv2.imread(img_path)\n",
    "#     augmented_images = data_aug(img) \n",
    "    \n",
    "#     for image in augmented_images:\n",
    "#         cv2.imwrite(os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0C48Lu5V4e4"
   },
   "source": [
    "# 3. Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6CmsHGxV4e4"
   },
   "source": [
    "## 3.1 Get Image Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ueXjXvqAV4e4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\3210195694.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpositive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive/Shareddrives/Face_Recognition/data/positive/*.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnegative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive/Shareddrives/Face_Recognition/data/negative2/*.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0manchor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive/Shareddrives/Face_Recognition/data/anchor/*.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "positive = tf.data.Dataset.list_files('/content/gdrive/Shareddrives/Face_Recognition/data/positive/*.jpg').take(300)\n",
    "negative = tf.data.Dataset.list_files('/content/gdrive/Shareddrives/Face_Recognition/data/negative2/*.jpg').take(300)\n",
    "anchor = tf.data.Dataset.list_files('/content/gdrive/Shareddrives/Face_Recognition/data/anchor/*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OMQdo2MCV4e4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anchor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\540326916.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdir_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'anchor' is not defined"
     ]
    }
   ],
   "source": [
    "dir_test = anchor.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVOk90XoV4e5",
    "outputId": "1a2b4436-3d28-4377-8124-5ce917ecf9ea"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dir_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\3829173974.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dir_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(dir_test.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtKCpRZEV4e6"
   },
   "source": [
    "## 3.2 Preprocessing - Scale and Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5llSzz9OV4e6"
   },
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # Load in the image \n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 100x100\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "\n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_tkIILYWV4e7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#img = preprocess('/content/gdrive/Shareddrives/Face_Recognition/data/anchor/1a18c9bc-64d8-11ed-904a-ac9084d4c809.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "rHlMmr9FSJra",
    "outputId": "c565c964-b449-4207-f21b-a2c1ddd4944f"
   },
   "outputs": [],
   "source": [
    "#plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYiwRkd4V4e7",
    "outputId": "258a9f6a-f29f-4f40-f826-4ea9a83e36e2"
   },
   "outputs": [],
   "source": [
    "#img.numpy().max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ClTInUchV4e7"
   },
   "outputs": [],
   "source": [
    "# dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EztvIf1UV4e8"
   },
   "source": [
    "## 3.3 Create Labelled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L6UPl02NV4e8"
   },
   "outputs": [],
   "source": [
    "# (anchor, positive) => 1,1,1,1,1\n",
    "# (anchor, negative) => 0,0,0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "R_fif2yAV4e8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\1010939526.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpositives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnegatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpositives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegatives\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "K6lAKaZqV4e8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\3569401717.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "samples = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FtaQ1LzIV4e9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\3244738351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexampple\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "exampple = samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHNvHYZoV4e9",
    "outputId": "9c9fd569-3961-47f3-b9c6-ec7f11d42b77"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exampple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\354502756.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexampple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'exampple' is not defined"
     ]
    }
   ],
   "source": [
    "exampple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd_9WP-0V4e9"
   },
   "source": [
    "## 3.4 Build Train and Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aJuxgixhV4e9"
   },
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "V8htlfZdV4e-"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exampple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\600348838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_twin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mexampple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'exampple' is not defined"
     ]
    }
   ],
   "source": [
    "res = preprocess_twin(*exampple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "NzewyMKnV4e-",
    "outputId": "3511d894-2200-41f2-faba-26b22f4cbdc2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\2043213977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhPqIx9qV4e-",
    "outputId": "0a0614c4-9440-4c94-c1bd-a8a3d12e88ec"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\4265798342.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hLk5c5FHV4e-"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\229244209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Build dataloader pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_twin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Build dataloader pipeline\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fg7cefktV4e_"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\3012365887.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training partition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3ks2hq-vV4e_"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25752\\1859265301.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing partition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8Y3TXL6V4e_"
   },
   "source": [
    "# 4. Model Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1adf2blpV4e_"
   },
   "source": [
    "## 4.1 Build Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "EW-qJSX6V4fA"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(100,100,3), name='input_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2p3KrzYUV4fA"
   },
   "outputs": [],
   "source": [
    "c1 = Conv2D(64, (10,10), activation='relu')(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dR7IgT41V4fA"
   },
   "outputs": [],
   "source": [
    "m1 = MaxPooling2D(64, (2,2), padding='same')(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "iyNh0bEdV4fA"
   },
   "outputs": [],
   "source": [
    "c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "m2 = MaxPooling2D(64, (2,2), padding='same')(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QB4N1pQVV4fA"
   },
   "outputs": [],
   "source": [
    "c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "m3 = MaxPooling2D(64, (2,2), padding='same')(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "b0PdDJmMV4fB"
   },
   "outputs": [],
   "source": [
    "c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "f1 = Flatten()(c4)\n",
    "d1 = Dense(4096, activation='sigmoid')(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "H-dUiWycV4fB"
   },
   "outputs": [],
   "source": [
    "mod = Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEDHBxhcV4fB",
    "outputId": "127d3036-a0e3-4aff-a188-22a1c409728f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     [(None, 100, 100, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 91, 91, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 40, 40, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 17, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "PDXCGzt_V4fB"
   },
   "outputs": [],
   "source": [
    "def make_embedding(): \n",
    "    inp = Input(shape=(100,100,3), name='input_image')\n",
    "    \n",
    "    # First block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    # Third block \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "o0CE-4b8V4fC"
   },
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Leq1vFQV4fC",
    "outputId": "6a777830-12ce-421f-df55-959b35e1819e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     [(None, 100, 100, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 91, 91, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 40, 40, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 17, 17, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb3ugV5nV4fD"
   },
   "source": [
    "## 4.2 Build Distance Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_image = Input(name='anchor_img', shape=(100,100,3))\n",
    "validation_image = Input(name='validation_img', shape=(100,100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_embedding = embedding(anchor_image)\n",
    "validation_embedding = embedding(validation_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "EXj1VkTBV4fD"
   },
   "outputs": [],
   "source": [
    "# Siamese L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "       \n",
    "    # Magic happens here - similarity calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "B8bdG0eBV4fD"
   },
   "outputs": [],
   "source": [
    "l1 = L1Dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "xzx8y9krV4fD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 4096) dtype=float32 (created by layer 'l1_dist_2')>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1(anchor_embedding, validation_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyuK9O9KV4fE"
   },
   "source": [
    "## 4.3 Make Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBrBWrG3V4fE"
   },
   "outputs": [],
   "source": [
    "input_image = Input(name='input_img', shape=(100,100,3))\n",
    "validation_image = Input(name='validation_img', shape=(100,100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSxafZBDV4fE"
   },
   "outputs": [],
   "source": [
    "inp_embedding = embedding(input_image)\n",
    "val_embedding = embedding(validation_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYa7NPpyV4fF"
   },
   "outputs": [],
   "source": [
    "siamese_layer = L1Dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezd74apgV4fF"
   },
   "outputs": [],
   "source": [
    "distances = siamese_layer(inp_embedding, val_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm-tob3GV4fF"
   },
   "outputs": [],
   "source": [
    "classifier = Dense(1, activation='sigmoid')(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAX2gfueV4fF",
    "outputId": "edb408a8-cabf-4bb5-d684-48bd831e6564"
   },
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Nzu8N6AV4fF"
   },
   "outputs": [],
   "source": [
    "siamese_network = Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okSJlWRdV4fF",
    "outputId": "8bd0582a-28e7-453a-b3e3-12283807fac8"
   },
   "outputs": [],
   "source": [
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9LJId4eV4fG"
   },
   "outputs": [],
   "source": [
    "def make_siamese_model(): \n",
    "    \n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "    \n",
    "    # Validation image in the network \n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "    \n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # Classification layer \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbyNH9KBV4fG"
   },
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "la3aZsmCV4fG",
    "outputId": "4595a2b4-1dc3-4975-dcf5-a4162c938187"
   },
   "outputs": [],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4Hh5gntV4fG"
   },
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ya-q53CYV4fG"
   },
   "source": [
    "## 5.1 Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWUmkWmfV4fH"
   },
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKM9XMdiV4fH"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMdJ6uZ8V4fH"
   },
   "source": [
    "## 5.2 Establish Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQglx-0hV4fH"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mB-bij9V4fH"
   },
   "source": [
    "## 5.3 Build Train Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSmpHMSjV4fH"
   },
   "outputs": [],
   "source": [
    "test_batch = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDjo02RUV4fI"
   },
   "outputs": [],
   "source": [
    "batch_1 = test_batch.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2q_lBMlZV4fI"
   },
   "outputs": [],
   "source": [
    "X = batch_1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fziDpSsV4fI"
   },
   "outputs": [],
   "source": [
    "y = batch_1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5mslRmyV4fI",
    "outputId": "c940bd55-b999-4cef-b2d2-5c79408778ef"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7xedrsLV4fI"
   },
   "outputs": [],
   "source": [
    "tf.losses.BinaryCrossentropy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPkRmS6ZV4fJ"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Record all of our operations \n",
    "    with tf.GradientTape() as tape:     \n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Calculate loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    print(loss)\n",
    "        \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "        \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvMZuCArV4fJ"
   },
   "source": [
    "## 5.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCVoF8ZUV4fJ"
   },
   "outputs": [],
   "source": [
    "# Import metric calculations\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfAawx89V4fK"
   },
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Creating a metric object \n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            loss = train_step(batch)\n",
    "            yhat = siamese_model.predict(batch[:2])\n",
    "            r.update_state(batch[2], yhat)\n",
    "            p.update_state(batch[2], yhat) \n",
    "            progbar.update(idx+1)\n",
    "        print(loss.numpy(), r.result().numpy(), p.result().numpy())\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwe-9VTQV4fK"
   },
   "source": [
    "## 5.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iLDblf7V4fK"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_39UNqvV4fK",
    "outputId": "ee03f55f-61e0-480d-8e9f-2e651e94ce60",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWHPTOeXV4fL"
   },
   "source": [
    "# 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-J6yvxJV4fL"
   },
   "source": [
    "## 6.1 Import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8Ss39XCV4fL"
   },
   "outputs": [],
   "source": [
    "# Import metric calculations\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osC6cl4aV4fL"
   },
   "source": [
    "## 6.2 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gm8o8fD0V4fL"
   },
   "outputs": [],
   "source": [
    "# Get a batch of test data\n",
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BU-qK8a_V4fL"
   },
   "outputs": [],
   "source": [
    "y_hat = siamese_model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uUKwRxmV4fM"
   },
   "outputs": [],
   "source": [
    "# Post processing the results \n",
    "[1 if prediction > 0.5 else 0 for prediction in y_hat ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCeQI1CYV4fM"
   },
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2vGLfJmV4fM"
   },
   "source": [
    "## 6.3 Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA0Wf-F0V4fM"
   },
   "outputs": [],
   "source": [
    "# Creating a metric object \n",
    "m = Recall()\n",
    "\n",
    "# Calculating the recall value \n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0xJD0KzV4fM"
   },
   "outputs": [],
   "source": [
    "# Creating a metric object \n",
    "m = Precision()\n",
    "\n",
    "# Calculating the recall value \n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3Qc6mxnV4fN"
   },
   "outputs": [],
   "source": [
    "r = Recall()\n",
    "p = Precision()\n",
    "\n",
    "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
    "    yhat = siamese_model.predict([test_input, test_val])\n",
    "    r.update_state(y_true, yhat)\n",
    "    p.update_state(y_true,yhat) \n",
    "\n",
    "print(r.result().numpy(), p.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D27RhTaWV4fN"
   },
   "source": [
    "## 6.4 Viz Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNuxyTJBV4fN"
   },
   "outputs": [],
   "source": [
    "# Set plot size \n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Set first subplot\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[0])\n",
    "\n",
    "# Set second subplot\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[0])\n",
    "\n",
    "# Renders cleanly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5ZD2zpJV4fO"
   },
   "source": [
    "# 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouZhoEV1V4fO"
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "siamese_model.save('siamesemodelv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-27cuZK1V4fO"
   },
   "outputs": [],
   "source": [
    "L1Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQUq5yg4V4fO"
   },
   "outputs": [],
   "source": [
    "# Reload model \n",
    "siamese_model = tf.keras.models.load_model('siamesemodelv1.h5', \n",
    "                                   custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DOdeuCaV4fO"
   },
   "outputs": [],
   "source": [
    "# Make predictions with reloaded model\n",
    "siamese_model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jJa8tSnV4fO"
   },
   "outputs": [],
   "source": [
    "# View model summary\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv9YBldUV4fP"
   },
   "source": [
    "# 8. Real Time Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3YYJJ2WV4fP"
   },
   "source": [
    "## 8.1 Verification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SA2idEmgxN74"
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWedtAjDV4fP"
   },
   "outputs": [],
   "source": [
    "os.listdir(os.path.join('application_data', 'verification_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tg2ugdVdV4fP"
   },
   "outputs": [],
   "source": [
    "#\n",
    "PATH = '/content/gdrive/my-drive'\n",
    "os.chdir(PATH)\n",
    "#\n",
    "os.path.join('application_data', 'input_image', 'input_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCHImT7wV4fP"
   },
   "outputs": [],
   "source": [
    "for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "    validation_img = os.path.join('application_data', 'verification_images', image)\n",
    "    print(validation_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlEJuPf8V4fQ"
   },
   "outputs": [],
   "source": [
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    # Build results array\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "        input_img = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "        validation_img = preprocess(os.path.join('application_data', 'verification_images', image))\n",
    "        \n",
    "        # Make Predictions \n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        results.append(result)\n",
    "    \n",
    "    # Detection Threshold: Metric above which a prediciton is considered positive \n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    \n",
    "    # Verification Threshold: Proportion of positive predictions / total positive samples \n",
    "    verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images'))) \n",
    "    verified = verification > verification_threshold\n",
    "    \n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysfXpY0PV4fQ"
   },
   "source": [
    "## 8.2 OpenCV Real Time Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5YaNkkOYj6Z"
   },
   "outputs": [],
   "source": [
    "results, verified = verify(siamese_model, 0.5, 0.5)\n",
    "print(verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVhxljyuV4fQ"
   },
   "outputs": [],
   "source": [
    "np.sum(np.squeeze(results) > 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjyR2IELV4fR"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vL6S9D02V4fR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ApLPHkXiV4e0",
    "1nYZ5FadV4e1",
    "n6CmsHGxV4e4",
    "UtKCpRZEV4e6",
    "EztvIf1UV4e8",
    "Qd_9WP-0V4e9",
    "1adf2blpV4e_",
    "pb3ugV5nV4fD",
    "wyuK9O9KV4fE",
    "ya-q53CYV4fG",
    "AMdJ6uZ8V4fH",
    "1mB-bij9V4fH",
    "dvMZuCArV4fJ",
    "fwe-9VTQV4fK",
    "x-J6yvxJV4fL",
    "osC6cl4aV4fL",
    "z2vGLfJmV4fM",
    "D27RhTaWV4fN",
    "e3YYJJ2WV4fP",
    "ysfXpY0PV4fQ"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
